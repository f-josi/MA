{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright: \n",
    "Prof. Dr. Christian Wartena, http://textmining.wp.hs-hannover.de/; Frieda Josi*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraktion von Wortpaaren\n",
    "\n",
    "Schlagworte können auch aus kurzen Phrasen (Wortpaaren) bestehen. Deshalb werden zusätzlich zu der Termextraktion auch Wortpaare exrahiert.\n",
    "\n",
    "Extraktion von Wortpaaren erfolgt aus den Beschriftungen und Referenzstellen der 397 Abbildungen aus Artikel, die aus Open Access Journals (NOA Projekt http://noa.wp.hs-hannover.de) stammen. Die Artikel haben meist mehrere Abbildungen, die aber nicht alle mit aufgenommen wurden. Es sind 397 einzelne txt-Dateien, angezeigt werden die Datensätze 150 bis 170. \n",
    "\n",
    "Kookkurenzen vom Wörter im gleichen Satz.\n",
    "Lemmata statt Wörter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((')', '.'), 1746), (('@card@', ')'), 1608), (('/math', '>'), 1167)]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import nltk\n",
    "import codecs\n",
    "import re\n",
    "import treetaggerwrapper\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "swlist = stopwords.words('english')\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='en')\n",
    "filelist = glob.glob(\"doi-cap-con/*.txt\")\n",
    "nr_of_words = 0\n",
    "fdist = nltk.FreqDist() #Worthäufigkeiten Anzahl\n",
    "fdist2 = nltk.FreqDist() #Wortpaarhäufigkeiten Frequenz von Wortpaaren\n",
    "\n",
    "for datei in filelist:  \n",
    "    textfile = codecs.open(datei, \"r\", \"utf-8\")\n",
    "    text = textfile.read()\n",
    "    textfile.close()\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(text,language='english')#Texte aufteilen, Jeder Satz ist eine Liste von Tokens\n",
    "    sentences_tok = [nltk.word_tokenize(sent,language='english') for sent in sentences]#Daraus Wortpaare und Wörter extrahieren\n",
    "    words = [] \n",
    "    for sent in sentences_tok:\n",
    "        tags = tagger.tag_text(sent,tagonly=True) \n",
    "        tags2 = treetaggerwrapper.make_tags(tags);\n",
    "        words = [lemma for (word,pos,lemma) in tags2 if pos[0]]#Jede Wortart+ Lemma\n",
    "        word_pairs = [] #Wortpaare\n",
    "        for i in range(len(tags2)-1) :#Bereich Liste von Zahlen 0 bis Länge von \"tags2\" -1. Das nächste letzte Wort soll immer angeschaut werden.\n",
    "            t1 = tags2[i]# Wort i + i+1, \n",
    "            t2 = tags2[i+1]\n",
    "            if t1.pos[0] and t2.pos[0]:\n",
    "                l1 = t1.lemma \n",
    "                l2 = t2.lemma\n",
    "                if l1 not in swlist and l2 not in swlist: #neu dazu geschrieben, Stopwörter rausnehmen\n",
    "                    word_pairs.append((l1,l2)) #10 häufigsten Wortpaare. Nur Häufigkeit, eigentlich ist die Kombination die häufig zu erwartet ist und auch eintrifft.\n",
    "                \n",
    "        nr_of_words += len(words)\n",
    "        fdist.update(words)\n",
    "        fdist2.update(word_pairs)\n",
    "        \n",
    "    \n",
    "print(fdist2.most_common(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eingrenzung auf ein Muster\n",
    "\n",
    "Wortartmuster für Kookkurrenzen (Wortartmuster für Nominalphrasen) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Wortvorkommen (tokens):  235570\n",
      "[(('time', 'series'), 69),\n",
      " (('wind', 'speed'), 69),\n",
      " (('standard', 'deviation'), 55),\n",
      " (('right', 'panel'), 53),\n",
      " (('radial', 'velocity'), 51),\n",
      " (('accumulation', 'rate'), 36),\n",
      " (('study', 'area'), 32),\n",
      " (('red', 'line'), 27),\n",
      " (('spindle', 'cell'), 26),\n",
      " (('water', 'level'), 26),\n",
      " (('blue', 'line'), 25),\n",
      " (('reference', 'blade'), 24),\n",
      " (('CTH', 'scheme'), 22),\n",
      " (('camera', 'motion'), 21),\n",
      " (('total', 'column'), 21),\n",
      " (('velocity', 'spectra'), 21),\n",
      " (('relative', 'humidity'), 20),\n",
      " (('mass', 'load'), 20),\n",
      " (('peritoneal', 'cavity'), 20),\n",
      " (('boundary', 'layer'), 20),\n",
      " (('ICEFLUX', 'scheme'), 20),\n",
      " (('diurnal', 'cycle'), 19),\n",
      " (('dental', 'pulp'), 18),\n",
      " (('transduction', 'efficiency'), 18),\n",
      " (('flash', 'rate'), 18),\n",
      " (('bone', 'marrow'), 18),\n",
      " (('tip', 'deflection'), 18),\n",
      " (('BARC', 'building'), 18),\n",
      " (('time', 'period'), 18),\n",
      " (('particulate', 'matter'), 17),\n",
      " (('lightning', 'activity'), 17),\n",
      " (('search', 'region'), 17),\n",
      " (('black', 'line'), 17),\n",
      " (('pressure', 'transducer'), 16),\n",
      " (('subperitoneal', 'space'), 16),\n",
      " (('Pareto', 'frontier'), 16),\n",
      " (('fluorescence', 'immunohistochemistry'), 16),\n",
      " (('spatial', 'distribution'), 16),\n",
      " (('=\\\\\\\\pm', '\\\\\\\\mathrm'), 15),\n",
      " (('ICEFLUX', 'approach'), 14),\n",
      " (('10Be', 'flux'), 14),\n",
      " (('atmospheric', 'stability'), 14),\n",
      " (('model', 'camera'), 14),\n",
      " (('shallow', 'layer'), 14),\n",
      " (('Soga', 'Best'), 13),\n",
      " (('VCD', '200m'), 13),\n",
      " (('left', 'panel'), 13),\n",
      " (('effective', 'area'), 13),\n",
      " (('optical', 'depth'), 13),\n",
      " (('correlation', 'coefficient'), 13),\n",
      " (('ice', 'core'), 12),\n",
      " (('study', 'site'), 12),\n",
      " (('speed', 'range'), 12),\n",
      " (('solid', 'line'), 12),\n",
      " (('vertical', 'distribution'), 12),\n",
      " (('objective', 'function'), 12),\n",
      " (('seasonal', 'cycle'), 12),\n",
      " (('wild', 'type'), 12),\n",
      " (('λ/2', '-resonance'), 12),\n",
      " (('Soga', 'Efficient'), 11),\n",
      " (('mode', 'shape'), 11),\n",
      " (('halogen', 'activation'), 11),\n",
      " (('aerosol', 'particle'), 11),\n",
      " (('sclerosing', 'RMS'), 11),\n",
      " (('global', 'bias'), 11),\n",
      " (('scalene', 'muscle'), 11),\n",
      " (('view', 'direction'), 11),\n",
      " (('Mount', 'St.'), 11),\n",
      " (('stimulation', 'amplitude'), 11),\n",
      " (('mean', 'wind'), 11),\n",
      " (('cell', 'RMS'), 11),\n",
      " (('emotional', 'closeness'), 11),\n",
      " (('inflammatory', 'response'), 11),\n",
      " (('air', 'temperature'), 11),\n",
      " (('cup', 'anemometer'), 11),\n",
      " (('cell', 'line'), 11),\n",
      " (('mean', 'bias'), 10),\n",
      " (('operational', 'wind'), 10),\n",
      " (('Park', 'Falls'), 10),\n",
      " (('molar', 'fraction'), 10),\n",
      " (('flapwise', 'mode'), 10),\n",
      " (('particle', 'extinction'), 10),\n",
      " (('mRNA', 'expression'), 10),\n",
      " (('nonminimal', 'coupling'), 10),\n",
      " (('upper', 'troposphere'), 10),\n",
      " (('surface', 'area'), 10),\n",
      " (('Snomax', 'mass'), 10),\n",
      " (('Mann', 'model'), 10),\n",
      " (('quantum', 'yield'), 10),\n",
      " (('image', 'pair'), 10),\n",
      " (('biofilm', 'formation'), 10),\n",
      " (('upper', 'panel'), 10),\n",
      " (('hot', 'spot'), 10),\n",
      " (('tensile', 'failure'), 10),\n",
      " (('MCM7', 'expression'), 10),\n",
      " (('particle', 'number'), 10),\n",
      " (('column', 'CO2'), 9),\n",
      " (('spindle-shaped', 'tumor'), 9),\n",
      " (('roughness', 'length'), 9),\n",
      " (('CO2', 'drawdown'), 9)]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import nltk\n",
    "import codecs\n",
    "import re\n",
    "import treetaggerwrapper\n",
    "import pprint\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "swlist = stopwords.words('english')\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='en')\n",
    "filelist = glob.glob(\"doi-cap-con/*.txt\")\n",
    "nr_of_words = 0\n",
    "fdist = nltk.FreqDist() #Worthäufigkeiten\n",
    "fdist2 = nltk.FreqDist() #Wortpaarhäufigkeiten\n",
    "\n",
    "\n",
    "npmuster = [('NN','NN'), #Nomen\n",
    "            ('NP','NP'), #Eigennamen\n",
    "            ('NN','NP'), \n",
    "            ('NP','NN'),\n",
    "            ('JJ','NP'), # Adjektive\n",
    "            ('JJ','NN')\n",
    "           ]\n",
    "    \n",
    "\n",
    "for datei in filelist:  \n",
    "    textfile = codecs.open(datei, \"r\", \"utf-8\")\n",
    "    text = textfile.read()\n",
    "    textfile.close()\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text,language='english')\n",
    "    sentences_tok = [nltk.word_tokenize(sent,language='english') for sent in sentences]\n",
    "    words = [] \n",
    "    for sent in sentences_tok:\n",
    "        tags = tagger.tag_text(sent,tagonly=True) \n",
    "        tags2 = treetaggerwrapper.make_tags(tags);\n",
    "        words = [lemma for (word,pos,lemma) in tags2 if pos[0]]\n",
    "        word_pairs = []\n",
    "        for i in range(len(tags2)-1) :\n",
    "            t1 = tags2[i]\n",
    "            t2 = tags2[i+1]\n",
    "            if (t1.pos,t2.pos) in npmuster:\n",
    "                l1 = t1.lemma\n",
    "                l2 = t2.lemma\n",
    "                if l1 not in swlist and l2 not in swlist  and len(l1) >2 and len(l2) >2: #Englische Stopwörter\n",
    "                    word_pairs.append((l1,l2))\n",
    "        \n",
    "        nr_of_words += len(words)\n",
    "        fdist.update(words)\n",
    "        fdist2.update(word_pairs)\n",
    "print(\"Anzahl Wortvorkommen (tokens): \", nr_of_words)\n",
    "pprint.pprint(fdist2.most_common(100))     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraktion von Nominalphrasen aus einzelne Datensätze\n",
    "\n",
    "Nominalphrasen nach Muster für Extraktion verwenden.\n",
    "Tokenisierung ändern: Häufigkeit und tf.idf Berechnung (Nominalphrase als einziges Token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import nltk\n",
    "import codecs\n",
    "import pprint\n",
    "import treetaggerwrapper\n",
    "\n",
    "df = {} \n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='en')\n",
    "NPList = fdist2.keys()\n",
    "\n",
    "def candidates(taglist):#Liste mit Tags, als Ausgabe als Liste\n",
    "    cand = []\n",
    "    skip = False\n",
    "    for i in range(len(taglist)-1) :\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        skip = False\n",
    "        l1 = taglist[i].lemma # erstes Wort\n",
    "        l2 = taglist[i+1].lemma# zweites Wort\n",
    "        if len(l1) >2 and len(l2) >2:   #Die einzelnen Wörter der NP sollen eine Länge von über 2 Buchstaben haben\n",
    "            if (l1,l2) in NPList:\n",
    "                skip = True\n",
    "                cand.append((l1,l2))\n",
    "    return cand\n",
    "\n",
    "\n",
    "def substantive_zaehlen(datei):\n",
    "    textfile = codecs.open(datei, \"r\", \"utf-8\")\n",
    "    text = textfile.read()\n",
    "    textfile.close()\n",
    "    \n",
    "    nouns_in_text = []   \n",
    "    sentences = nltk.sent_tokenize(text,language='english')\n",
    "    sentences_tok = [nltk.word_tokenize(sent,language='english') for sent in sentences]\n",
    "    for sent in sentences_tok:\n",
    "        tags = tagger.tag_text(sent,tagonly=True) \n",
    "        tags2 = treetaggerwrapper.make_tags(tags);\n",
    "        nouns_from_sent = candidates(tags2)\n",
    "        for substantiv in nouns_from_sent:\n",
    "            if substantiv not in nouns_in_text:\n",
    "                nouns_in_text.append(substantiv)\n",
    "                \n",
    "    for n in nouns_in_text:\n",
    "        df_n = df.get(n,0) #Der Wert von n falls n vorhanden ist, sonst 0\n",
    "        df[n] = df_n + 1\n",
    "\n",
    "for f in glob.glob(\"doi-cap-con/*.txt\"):\n",
    "    substantive_zaehlen(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nominalphrasen für einen Datensatz extrahieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'doi-cap-con/10.1155:2016:1875357.txt'\n",
      "[(('time', 'slot'), 0.9106192449462366),\n",
      " (('different', 'wavelength'), 0.9106192449462366),\n",
      " (('stripe', 'size'), 0.9106192449462366),\n",
      " (('switch', 'fabric'), 0.6070794966308244),\n",
      " (('signal', 'conversion'), 0.3035397483154122),\n",
      " (('single', 'bit'), 0.3035397483154122),\n",
      " (('semisynchronous', 'time'), 0.3035397483154122),\n",
      " (('electronic', 'control'), 0.3035397483154122)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:1901493.txt'\n",
      "[(('internal', 'combustion'), 0.46542761408363204),\n",
      " (('predefined', 'speed'), 0.46542761408363204),\n",
      " (('valve', 'train'), 0.46542761408363204),\n",
      " (('boundary', 'lubrication'), 0.46542761408363204),\n",
      " (('four-ball', 'tribotester'), 0.46542761408363204),\n",
      " (('acquisition', 'software'), 0.46542761408363204),\n",
      " (('Four-ball', 'tribotester'), 0.46542761408363204),\n",
      " (('top', 'ball'), 0.46542761408363204)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:1925827.txt'\n",
      "[(('lattice', 'rectangle'), 0.7480086654915514),\n",
      " (('random', 'walk'), 0.4986724436610343),\n",
      " (('corner', 'condition'), 0.4986724436610343),\n",
      " (('state', 'space'), 0.4986724436610343),\n",
      " (('joint', 'distribution'), 0.4986724436610343),\n",
      " (('significant', 'mass'), 0.24933622183051715),\n",
      " (('Steady', 'state'), 0.24933622183051715),\n",
      " (('main', 'balance'), 0.24933622183051715)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:1928465.txt'\n",
      "[(('optical', 'light'), 2.32713807041816),\n",
      " (('BOOTES', 'observation'), 1.16356903520908),\n",
      " (('first', 'telescope'), 1.16356903520908),\n",
      " (('optical', 'transient'), 1.16356903520908),\n",
      " (('Table', '@card@'), 0.47842339118019483)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:1962438.txt'\n",
      "[(('panel', 'frame'), 0.8726767764068101),\n",
      " (('bus', 'voltage'), 0.8726767764068101),\n",
      " (('grid', 'voltage'), 0.8726767764068101),\n",
      " (('parasitic', 'capacitance'), 0.8726767764068101),\n",
      " (('capacitance', 'value'), 0.8726767764068101),\n",
      " (('single-phase', 'inverter'), 0.8726767764068101),\n",
      " (('Schematic', 'diagram'), 0.6993899812668237),\n",
      " (('schematic', 'diagram'), 0.6714970373525475)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:1979348.txt'\n",
      "[(('joint', 'disease'), 0.8726767764068101),\n",
      " (('Chondrofix', 'plug'), 0.8726767764068101),\n",
      " (('MFC', 'OCD'), 0.8726767764068101),\n",
      " (('resultant', 'instability'), 0.8726767764068101),\n",
      " (('range', '18–59'), 0.8726767764068101),\n",
      " (('cruciate', 'ligament'), 0.7860333788368169),\n",
      " (('average', 'age'), 0.7353502403232963),\n",
      " (('Table', '@card@'), 0.35881754338514615)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2136381.txt'\n",
      "[(('3-month', 'follow-up'), 0.9973448873220686),\n",
      " (('Table', '@card@'), 0.820154384880334),\n",
      " (('follow-up', 'assessment'), 0.4986724436610343),\n",
      " (('analog', 'pain'), 0.4986724436610343),\n",
      " (('between-group', 'comparison'), 0.4986724436610343),\n",
      " (('preprocedural', 'pain'), 0.4986724436610343),\n",
      " (('treatment', 'group'), 0.4491619307638953),\n",
      " (('study', 'period'), 0.3996514178667564)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2175896.txt'\n",
      "[(('time', 'curve'), 1.7453535528136201),\n",
      " (('elimination', 'half-life'), 0.8726767764068101),\n",
      " (('Plasma', 'concentration'), 0.8726767764068101),\n",
      " (('prazosin', 'pretreatment'), 0.8726767764068101),\n",
      " (('maximum', 'concentration'), 0.7860333788368169),\n",
      " (('significant', 'effect'), 0.6714970373525475),\n",
      " (('Table', '@card@'), 0.35881754338514615)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2303181.txt'\n",
      "[(('input', 'variable'), 0.41067142419144004),\n",
      " (('synaptic', 'weight'), 0.41067142419144004),\n",
      " (('network', 'architecture'), 0.20533571209572002),\n",
      " (('Case', 'Study'), 0.20533571209572002),\n",
      " (('line', 'thickness'), 0.20533571209572002),\n",
      " (('feed-forward', 'backpropagation'), 0.20533571209572002),\n",
      " (('unbiased', 'estimation'), 0.20533571209572002),\n",
      " (('output', 'activation'), 0.20533571209572002)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2315949.txt'\n",
      "[(('Polycyclic', 'Aromatic'), 1.904022057614858),\n",
      " (('graphene', 'sheet'), 1.2693480384099056),\n",
      " (('infinite', 'PAH'), 0.6346740192049528),\n",
      " (('Hydrocarbons', 'PAHk'), 0.6346740192049528),\n",
      " (('general', 'representation'), 0.6346740192049528),\n",
      " (('PAH', 'molecule'), 0.6346740192049528),\n",
      " (('successful', 'utilization'), 0.6346740192049528),\n",
      " (('molecular', 'family'), 0.6346740192049528)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2350615.txt'\n",
      "[(('Seribu', 'Island'), 1.9946897746441372),\n",
      " (('Research', 'location'), 0.9973448873220686),\n",
      " (('Instrumentation', 'Laboratory'), 0.9973448873220686),\n",
      " (('Bogor', 'Agricultural'), 0.9973448873220686),\n",
      " (('Ocean', 'Acoustics'), 0.9973448873220686),\n",
      " (('Marine', 'Science'), 0.9973448873220686)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2380540.txt'\n",
      "[(('cell', 'line'), 0.6920943438336906),\n",
      " (('possible', 'DTD'), 0.41067142419144004),\n",
      " (('ionic', 'interaction'), 0.41067142419144004),\n",
      " (('human', 'tumor'), 0.41067142419144004),\n",
      " (('monodentate', 'fashion'), 0.41067142419144004),\n",
      " (('several', 'tumor'), 0.41067142419144004),\n",
      " (('low', 'distortion'), 0.41067142419144004),\n",
      " (('anionic', 'polymer'), 0.41067142419144004)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2385429.txt'\n",
      "[(('students’', 'feedback'), 0.7757126901393867),\n",
      " (('overall', 'process'), 0.7757126901393867),\n",
      " (('sentiment', 'score'), 0.7757126901393867),\n",
      " (('name', 'entity'), 0.7757126901393867),\n",
      " (('sentiment', 'analysis'), 0.7757126901393867),\n",
      " (('major', 'building'), 0.7757126901393867),\n",
      " (('word', 'cloud'), 0.7757126901393867),\n",
      " (('data', 'gathering'), 0.7757126901393867)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2409521.txt'\n",
      "[(('additional', 'test'), 1.2320142725743202),\n",
      " (('minimal', 'fault'), 1.2320142725743202),\n",
      " (('simulated', 'Experiment'), 0.8213428483828801),\n",
      " (('simulated', 'experiment'), 0.8213428483828801),\n",
      " (('input', 'model'), 0.41067142419144004),\n",
      " (('interactions’', 'count'), 0.41067142419144004),\n",
      " (('safe', 'value'), 0.41067142419144004),\n",
      " (('special', 'value'), 0.41067142419144004)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2458685.txt'\n",
      "[(('Vickers', 'hardness'), 0.6494338801166959),\n",
      " (('second', 'phase'), 0.4870754100875219),\n",
      " (('main', 'phase'), 0.4870754100875219),\n",
      " (('tetragonal', 'zirconia'), 0.32471694005834795),\n",
      " (('pure', 'ZrO'), 0.32471694005834795),\n",
      " (('phase', 'transformation'), 0.32471694005834795),\n",
      " (('holding', 'time'), 0.32471694005834795),\n",
      " (('different', 'Tio'), 0.32471694005834795)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2606453.txt'\n",
      "[(('handheld', 'cradle'), 0.5475618989219201),\n",
      " (('iPod', 'cradle'), 0.27378094946096004),\n",
      " (('resection', 'plane'), 0.27378094946096004),\n",
      " (('distal', 'femur'), 0.2465987070860602),\n",
      " (('femoral', 'coronal'), 0.13689047473048002),\n",
      " (('tibial', 'coronal'), 0.13689047473048002),\n",
      " (('flexion', 'alignment'), 0.13689047473048002),\n",
      " (('lateral', 'malleolus'), 0.13689047473048002)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2642361.txt'\n",
      "[(('SCD', 'crisis'), 0.6648965915480457),\n",
      " (('clustered', 'HbS'), 0.33244829577402285),\n",
      " (('large', 'field'), 0.33244829577402285),\n",
      " (('microvascular', 'bed'), 0.33244829577402285),\n",
      " (('simultaneous', 'detection'), 0.33244829577402285),\n",
      " (('early', 'warning'), 0.33244829577402285),\n",
      " (('new', 'perspective'), 0.33244829577402285),\n",
      " (('vivo', 'detection'), 0.33244829577402285)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2653915.txt'\n",
      "[(('Gait', 'analysis'), 0.5370318624041909),\n",
      " (('Rehabilitation', 'Centre'), 0.5370318624041909),\n",
      " (('ethical', 'approval'), 0.5370318624041909),\n",
      " (('gait', 'laboratory'), 0.5370318624041909),\n",
      " (('paretic', 'leg'), 0.5370318624041909),\n",
      " (('first', 'session'), 0.5370318624041909),\n",
      " (('physical', 'rehabilitation'), 0.5370318624041909),\n",
      " (('West', 'Midlands'), 0.5370318624041909)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2794364.txt'\n",
      "[(('serum', 'dsDNA'), 1.46977141289568),\n",
      " (('total', 'cohort'), 0.73488570644784),\n",
      " (('dsDNA', 'concentration'), 0.73488570644784),\n",
      " (('off-pump', 'surgery'), 0.73488570644784),\n",
      " (('on-pump', 'surgery'), 0.36744285322392),\n",
      " (('PostOp', 'serum'), 0.36744285322392),\n",
      " (('PostOp', 'median'), 0.36744285322392),\n",
      " (('double-strand', 'DNA'), 0.36744285322392)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2937632.txt'\n",
      "[(('wall-sized', 'video'), 0.3173370096024764),\n",
      " (('additional', 'simulator'), 0.3173370096024764),\n",
      " (('Unity', 'game'), 0.3173370096024764),\n",
      " (('second', 'device'), 0.3173370096024764),\n",
      " (('skeletal', 'structure'), 0.3173370096024764),\n",
      " (('car', 'skeleton'), 0.3173370096024764),\n",
      " (('laboratory', 'wall'), 0.3173370096024764),\n",
      " (('two-dimensional', 'projection'), 0.3173370096024764)]\n",
      "'------------------------------'\n"
     ]
    }
   ],
   "source": [
    "def extract_kw(datei,nr_of_docs):\n",
    "    tagger = treetaggerwrapper.TreeTagger(TAGLANG='en')\n",
    "    \n",
    "    textfile = codecs.open(datei, \"r\", \"utf-8\")\n",
    "    text = textfile.read()\n",
    "    textfile.close()\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(text,language='english')\n",
    "    sentences_tok = [nltk.word_tokenize(sent,language='english') for sent in sentences]\n",
    "    nouns = [] \n",
    "    for sent in sentences_tok:\n",
    "        tags = tagger.tag_text(sent,tagonly=True)\n",
    "        tags2 = treetaggerwrapper.make_tags(tags);\n",
    "        nouns_from_sent = candidates(tags2)\n",
    "        nouns.extend(nouns_from_sent)\n",
    "\n",
    "    fdist = nltk.FreqDist(nouns)  \n",
    "    \n",
    "        \n",
    "    for word in fdist:\n",
    "        idf = 1.0 + math.log(float(nr_of_docs) / float(df[word]))\n",
    "        fdist[word] = float(fdist[word]) / float(len(nouns)) * idf\n",
    "            \n",
    "    return fdist.most_common(8)\n",
    "    \n",
    "\n",
    "filelist = glob.glob(\"doi-cap-con/*.txt\")\n",
    "nr_of_docs = len(filelist)\n",
    "for f in filelist[150:170]:\n",
    "    keywords = extract_kw(f,nr_of_docs)\n",
    "    pprint.pprint(f)\n",
    "    pprint.pprint(keywords)\n",
    "    pprint.pprint(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kategorien Mapping\n",
    "8 beste Wortpaare\n",
    "Nominalphrase als String ohne Sonderzeichen: ,  ) ( '  in der Funktion \"def wiki_cats(term)\" \n",
    "Umwandeln in Kleinbuchstaben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'doi-cap-con/10.1155:2016:1875357.txt'\n",
      "set()\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:1901493.txt'\n",
      "set()\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:1925827.txt'\n",
      "{'Category:Artificial intelligence stubs',\n",
      " 'Category:Dynamical systems',\n",
      " 'Category:Models of computation',\n",
      " 'Category:Stochastic processes',\n",
      " 'Category:Systems theory',\n",
      " 'Category:Variants of random walks'}\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:1928465.txt'\n",
      "set()\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:1962438.txt'\n",
      "{'Category:Capacitors', 'Category:Capacitance'}\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:1979348.txt'\n",
      "{'Category:Ligaments'}\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2136381.txt'\n",
      "set()\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2175896.txt'\n",
      "set()\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2303181.txt'\n",
      "{'Category:Artificial neural networks',\n",
      " 'Category:Computer science stubs',\n",
      " 'Category:Evaluation methods',\n",
      " 'Category:Evidence',\n",
      " 'Category:Network architecture',\n",
      " 'Category:Neural networks',\n",
      " 'Category:Neuroplasticity',\n",
      " 'Category:Neuroscience stubs',\n",
      " 'Category:Scientific method',\n",
      " 'Category:Telecommunications engineering'}\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2315949.txt'\n",
      "set()\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2350615.txt'\n",
      "set()\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2380540.txt'\n",
      "set()\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2385429.txt'\n",
      "{'Category:Affective computing',\n",
      " 'Category:Natural language processing',\n",
      " 'Category:Polling',\n",
      " 'Category:Social media'}\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2409521.txt'\n",
      "set()\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2458685.txt'\n",
      "set()\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2606453.txt'\n",
      "set()\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2642361.txt'\n",
      "set()\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2653915.txt'\n",
      "{'Category:Biometrics',\n",
      " 'Category:Forensic disciplines',\n",
      " 'Category:Forensic techniques',\n",
      " 'Category:Neurology procedures',\n",
      " 'Category:Orthopedic surgical procedures',\n",
      " 'Category:Rehabilitation medicine',\n",
      " 'Category:Terrestrial locomotion'}\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2794364.txt'\n",
      "set()\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2937632.txt'\n",
      "set()\n",
      "'------------------------------'\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import nltk\n",
    "import codecs\n",
    "import pprint\n",
    "import treetaggerwrapper\n",
    "import math\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "import re\n",
    "\n",
    "filelist = glob.glob(\"doi-cap-con/*.txt\") #glob-Modul für Pfadnamen nach Muster\n",
    "nr_of_docs = len(filelist) #Anzahl der Dateien\n",
    "noun2cat = {} #Dictionary für Substantiv -> [Wikicats]\n",
    "catStoplist = stopwords.words('categories') ####unsinnige Kategorien (z.B Numbers)\n",
    "df = {} \n",
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='en')\n",
    "NPList = fdist2.keys()\n",
    "\n",
    "def candidates(taglist):#Liste mit Tags, als Ausgabe als Liste\n",
    "    cand = []\n",
    "    skip = False\n",
    "    for i in range(len(taglist)-1) :\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        skip = False\n",
    "        l1 = taglist[i].lemma # erstes Wort\n",
    "        l2 = taglist[i+1].lemma# zweites Wort\n",
    "        if len(l1) >2 and len(l2) >2:   #Die einzelnen Wörter der NP sollen eine Länge von über 2 Buchstaben haben\n",
    "            if (l1,l2) in NPList:\n",
    "                skip = True\n",
    "                cand.append((l1,l2))\n",
    "    return cand\n",
    "\n",
    "\n",
    "def substantive_zaehlen(datei):\n",
    "    textfile = codecs.open(datei, \"r\", \"utf-8\")\n",
    "    text = textfile.read()\n",
    "    textfile.close()\n",
    "    \n",
    "    nouns_in_text = []   \n",
    "    sentences = nltk.sent_tokenize(text,language='english')\n",
    "    sentences_tok = [nltk.word_tokenize(sent,language='english') for sent in sentences]\n",
    "    for sent in sentences_tok:\n",
    "        tags = tagger.tag_text(sent,tagonly=True) \n",
    "        tags2 = treetaggerwrapper.make_tags(tags);\n",
    "        nouns_from_sent = candidates(tags2)\n",
    "        for substantiv in nouns_from_sent:\n",
    "            if substantiv not in nouns_in_text:\n",
    "                nouns_in_text.append(substantiv)\n",
    "                \n",
    "    for n in nouns_in_text:\n",
    "        df_n = df.get(n,0) #Der Wert von n falls n vorhanden ist, sonst 0\n",
    "        df[n] = df_n + 1\n",
    "    \n",
    "\n",
    "for f in glob.glob(\"doi-cap-con/*.txt\"):\n",
    "    substantive_zaehlen(f)\n",
    "    \n",
    "def extract_kw(datei,nr_of_docs):\n",
    "    tagger = treetaggerwrapper.TreeTagger(TAGLANG='en')\n",
    "    \n",
    "    textfile = codecs.open(datei, \"r\", \"utf-8\")\n",
    "    text = textfile.read()\n",
    "    textfile.close()\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(text,language='english')\n",
    "    sentences_tok = [nltk.word_tokenize(sent,language='english') for sent in sentences]\n",
    "    nouns = [] \n",
    "    for sent in sentences_tok:\n",
    "        tags = tagger.tag_text(sent,tagonly=True) \n",
    "        tags2 = treetaggerwrapper.make_tags(tags);\n",
    "        nouns_from_sent = candidates(tags2)\n",
    "        nouns.extend(nouns_from_sent)\n",
    "    fdist = nltk.FreqDist(nouns)  \n",
    "    \n",
    "    for word in fdist:\n",
    "        idf = 1.0 + math.log(float(nr_of_docs) / float(df[word]))\n",
    "        fdist[word] = float(fdist[word]) / float(len(nouns)) * idf\n",
    "        \n",
    "    \n",
    "    return fdist.most_common(8)\n",
    "\n",
    "def wiki_cats(term):\n",
    "    cats = []    \n",
    "    term_string = str(term)  #Strings, damit die Zeichen ,')( entfernt werden können\n",
    "    term_lower = term_string.lower() #Kleinbuchstaben, weil zweites Wort bei Wikipedia Titel klein beginnen muss\n",
    "    rechar = re.compile(r\"[',()]\") #Ersetzen der Zeichen ,')(  zu einem Leerzeichen\n",
    "    term_clear = rechar.sub(\"\", term_lower)\n",
    "     \n",
    "    response = requests.get(\n",
    "        'https://en.wikipedia.org/w/api.php',\n",
    "        params={\n",
    "            'action': 'query',\n",
    "            'format': 'json',\n",
    "            'titles': term_clear,  ### Bereinigte Nominalphrasen\n",
    "            'prop': 'categories',\n",
    "            'clshow':'!hidden',\n",
    "            'cllimit':'50'\n",
    "        }\n",
    "    ).json()    \n",
    "    \n",
    "    for pageid in response['query']['pages']:\n",
    "        if 'categories' in response['query']['pages'][pageid]: \n",
    "            cats.extend([cat['title'] for cat in response['query']['pages'][pageid]['categories'] if cat['ns'] == 14 and not cat['title'].startswith('Category:Disambiguation') and not cat['title'].startswith('Category:Wikipedia articles incorporating') and cat['title'] not in catStoplist])\n",
    "        return set(cats) \n",
    "\n",
    "def collect_nouns(flist):\n",
    "    global noun2cat\n",
    "    for f in flist:\n",
    "        nouns = extract_kw(f, nr_of_docs)\n",
    "        for (n,f) in nouns:   \n",
    "            noun2cat[n] = wiki_cats(n)   \n",
    "\n",
    "collect_nouns(filelist[150:170])\n",
    "\n",
    "\n",
    "filelist = glob.glob(\"doi-cap-con/*.txt\")\n",
    "nr_of_docs = len(filelist)\n",
    "for f in filelist[150:170]:\n",
    "    keywords = extract_kw(f,nr_of_docs)   \n",
    "    catsPair = set([c for (kw,f) in keywords for c in noun2cat.get(kw)]) \n",
    "    pprint.pprint(f)\n",
    "    pprint.pprint(catsPair)\n",
    "    pprint.pprint(\"------------------------------\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking der Kategorien\n",
    "Oberkategorien der Kategorien ermittel. Anzahl der Überscheidungen = Anzahl Beziehungen\n",
    "Ranking nach Anzahl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'doi-cap-con/10.1155:2016:1875357.txt'\n",
      "[]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:1901493.txt'\n",
      "[]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:1925827.txt'\n",
      "[('Category:Systems theory', 1),\n",
      " ('Category:Stochastic processes', 1),\n",
      " ('Category:Variants of random walks', 1),\n",
      " ('Category:Dynamical systems', 1),\n",
      " ('Category:Models of computation', 0),\n",
      " ('Category:Artificial intelligence stubs', 0)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:1928465.txt'\n",
      "[]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:1962438.txt'\n",
      "[('Category:Capacitors', 1), ('Category:Capacitance', 1)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:1979348.txt'\n",
      "[('Category:Ligaments', 0)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2136381.txt'\n",
      "[]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2175896.txt'\n",
      "[]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2303181.txt'\n",
      "[('Category:Neuroscience stubs', 2),\n",
      " ('Category:Computer science stubs', 1),\n",
      " ('Category:Neural networks', 1),\n",
      " ('Category:Artificial neural networks', 1),\n",
      " ('Category:Scientific method', 1),\n",
      " ('Category:Neuroplasticity', 1),\n",
      " ('Category:Evaluation methods', 1),\n",
      " ('Category:Evidence', 0),\n",
      " ('Category:Network architecture', 0),\n",
      " ('Category:Telecommunications engineering', 0)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2315949.txt'\n",
      "[]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2350615.txt'\n",
      "[]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2380540.txt'\n",
      "[]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2385429.txt'\n",
      "[('Category:Polling', 0),\n",
      " ('Category:Social media', 0),\n",
      " ('Category:Affective computing', 0),\n",
      " ('Category:Natural language processing', 0)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2409521.txt'\n",
      "[]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2458685.txt'\n",
      "[]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2606453.txt'\n",
      "[]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2642361.txt'\n",
      "[]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2653915.txt'\n",
      "[('Category:Forensic disciplines', 2),\n",
      " ('Category:Forensic techniques', 1),\n",
      " ('Category:Biometrics', 1),\n",
      " ('Category:Orthopedic surgical procedures', 0),\n",
      " ('Category:Rehabilitation medicine', 0),\n",
      " ('Category:Terrestrial locomotion', 0),\n",
      " ('Category:Neurology procedures', 0)]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2794364.txt'\n",
      "[]\n",
      "'------------------------------'\n",
      "'doi-cap-con/10.1155:2016:2937632.txt'\n",
      "[]\n",
      "'------------------------------'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def wiki_cats(term):\n",
    "    cats = []\n",
    "    response = requests.get(\n",
    "        'https://en.wikipedia.org/w/api.php',\n",
    "        params={\n",
    "            'action': 'query',\n",
    "            'format': 'json',\n",
    "            'titles': term,\n",
    "            'prop': 'categories',\n",
    "            'clshow':'!hidden',\n",
    "            'cllimit':'50'\n",
    "        }\n",
    "    ).json()\n",
    "    \n",
    "    for pageid in  response['query']['pages']:\n",
    "        cats.extend([cat['title'] for cat in response['query']['pages'][pageid]['categories'] if cat['ns'] == 14])\n",
    "\n",
    "    return cats # Es sind doppelte Kategorien vorhanden\n",
    "\n",
    "\n",
    "def intersection(a,b):\n",
    "    i = 0\n",
    "    for e in a:\n",
    "        if e in b:\n",
    "            i += 1\n",
    "    return i\n",
    "\n",
    "def wiki_cats_sort_rel(f):\n",
    "    keywords = extract_kw(f,nr_of_docs) \n",
    "    cats = set([c for (kw,f) in keywords for c in noun2cat.get(kw)]) \n",
    "    extended = {}\n",
    "    relnumber = {}\n",
    "    for cat in cats:\n",
    "        family = [cat]\n",
    "        family.extend(wiki_cats(cat))\n",
    "        extended[cat] = family\n",
    "\n",
    "    for cat in cats:\n",
    "        relsize = 0\n",
    "        for cat2 in cats:\n",
    "            if cat == cat2:\n",
    "                continue\n",
    "            if intersection(extended[cat],extended[cat2]) > 0:\n",
    "                relsize +=1\n",
    "        relnumber[cat] = relsize\n",
    "    return sorted(relnumber.items(),key = lambda x:x[1],reverse=True)\n",
    "            \n",
    "for f in filelist[150:170]:    \n",
    "    pprint.pprint(f) \n",
    "    cts = wiki_cats_sort_rel(f)\n",
    "    pprint.pprint(cts)\n",
    "    pprint.pprint(\"------------------------------\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
